\documentclass{jsarticle}
\begin{document}

\title{Non Stochastic Dynamic Programming＿＿学習の手引き＿＿}
\author{池上慧}
\maketitle

\begin{abstract}
この学習の手引きは、DP学習がスムーズに進むように作成されたものである。理論上重要な証明が載っている文献をネットから拾ってきただけなので偉そうなことは言えないが、注釈に記したページ数に飛べば定理等の証明をすぐに参照できるようになっている。一応池上は一通り証明もこなしたので、わからなかったら聞いてください。また、最後の方に、経済学とは直接関係ないが、数理最適化問題の解法としてはDPがどのように使われるかがよくわかる例を乗せたのでこちらもご参照ください。iterationが何を行っているのかがよくわかる良い例だと思います。とりあえず京大の経済学のための数学を読めばどういう話なのかわかります。iterationのくだりは縮小写像原理とかあって難しいので、東工大の例でイメージを掴んで、京大の証明をみてください。今回はnon stochasticなDPを扱いますが、stochastic編とiterationのpythonプログラミングコード編も作ろうと思います。
\end{abstract}

\section{DP概説}
動的計画法（Dynamic Programming）は、無限期間最適化問題の解法のことである。一般に、無限期間最適化問題が最終的に求めたい物は、無限期間での効用最大化を達成するために取るべき最適行動の列である。ここでDPは、直接に行動の列を算出することで問題を解こうとするのではなく、現在の意思決定を一期前の行動を変数とした関数で表すためにはどのような関数であればよいか、を特定することで
この問題を解こうというものである。目的関数が微分可能であれば、オイラー方程式と横断性条件を使って同様の無限期間最適化問題を解くことができる\footnote{京大 75から82}が、そのような仮定を置くことができない場合の一般的な解法としてベルマン方程式を使って解くDPが存在する、と考えると良い。このような事情のため、動的計画法の解説にはオイラー方程式や横断性条件との関係性\footnote{京大 88、動学最適化 8}が言及されることも多いが、とりあえずDPを解くということに関してはそれらは関係なく、ベルマン方程式さえ解ければ良い。

ベルマン方程式を解けばDPが解ける\footnote{京大 82~86、動学最適化 5~8、}、ということを理解した次に学ぶべきはベルマン方程式の解き方である。解法として抑えておきたいのは以下の三つである。
\begin{itemize}
\item Guess and Verify\footnote{マクロ 10から11}
\item Value function iteration\footnote{マクロ 8から9}
\item Policy function iteration\footnote{マクロ 11から13}
\end{itemize}
上から順に説明していくと、最初のGuess and Verifyはvalue functionの形を山勘で当てて、それがベルマン方程式の解となっていることを確認するというものだ。数学的帰納法で漸化式を解く際に山勘で一般項の形を当てはめてそれを正当化する解法が大学受験の際にあったが、あれとやっていることは同じである。次にvalue function iterationである。これが最も基本的なベルマン方程式の解法である。value functionの形を適当に推測し、ベルマン方程式に代入、出力結果を再びベルマン方程式に代入し、といった具合に、入力と出力がほとんど同じになるまでこれを繰り返すことでvalue functionを求めるやり方。これによってvelue functionの形に近づいていけることの証明\footnote{京大 86から87}。最後がpolicy function iterationであり、これはvalue functionとpolicy functionとの間でiterationを行うアルゴリズム。policy functionが入力と出力であまり変化しなくなるまで続ける。この解法は収束が早いことで知られているが、policy functionの真の形に近づくことの証明はなされていない。

\section{用語説明}
\begin{itemize}
\setlength{\leftskip}{2.0cm}
\item[Policy function]  日本語では政策関数。DPを解く際に求めたい物であるのは先に述べた通り、一期前の行動と現在の意思決定とをつなげる関数であり、この求めたい関数をpolicy functionと呼ぶ。

\item[Value function]  日本語では価値関数。定義としては初期値を変数として目的関数の最大値を表す関数である。すなわち無限期間最適化問題において最大化された目的関数（を初期値の関数として表現したもの）のことである。最適性の原理（後述）により、ある期における状態を入力すると、その状態から達成しうる最大の利得を出力する関数と考えることができる。

\item[Bellman equation]  今期の状態を入力すると、今期の行動によって得られる値と今期の行動によって決定される次期の状態を同じ関数に入力して得られる値との和を最大にするような今期の行動を探しだし、その行動をとって得られた最大値を出力するような関数を、解として算出する関数方程式のことである。最適性の原理とは、先のvalue functionがこのbellman equationの解となることを示す定理、すなわち無限期間最適化問題の解をベルマン方程式が与えるということである\footnote{京大 82から86}。
\end{itemize}

\section{諸注意と参照もと}
この学習の手引きは以下のpdfファイルを参考にしている。また、各種証明はtexで打つあまりのめんどくささに挫折したので、以下のpdfファイルから切り取ってきている。
\begin{itemize}
\item 経済学のための数学、京都大学経済学研究所、原千秋・梶井厚志
\item 動学的最適化入門、工藤教考
\item 2013年度夏学期　上級マクロ経済学講義ノート：動的計画法、阿部修人
\item 動的計画法、東工大
\end{itemize}
特に京大の経済学のための数学は便利であった。

また、このように様々な文献から証明部分を切りはりしているので、問題の定式化が異なっていることが多々ある。注意深く読めば問題ないはずだが、の書き方にふた通りあることには特に注意が必要だ。京大やStokey and Lucasが採用しているのが利得関数を現在の状態と、次の状態の関数で表されている形態である。こちらは、ベルマン方程式の中に行動という変数（操作変数が出てこない）。二つ目が利得関数を現在の状態と今の意思決定の関数として表したもので、こちらは工藤などが採用している。慣れてくると、この違いは気にならなくなるので、なれるまで読みましょう。


\end{document}