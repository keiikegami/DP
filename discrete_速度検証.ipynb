{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# LPをパーツごとに時間計測する\n",
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import quantecon as qe\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "import time\n",
    "\n",
    "class SimpleOG(object):\n",
    "\n",
    "    def __init__(self, B=10, M=5, alpha=0.5, beta=0.9):\n",
    "\n",
    "        self.B, self.M, self.alpha, self.beta  = B, M, alpha, beta\n",
    "        self.n = B + M + 1\n",
    "        self.m = M + 1\n",
    "\n",
    "        self.R = np.empty((self.n, self.m))\n",
    "        self.Q = np.zeros((self.n, self.m, self.n))\n",
    "\n",
    "        self.populate_Q()\n",
    "        self.populate_R()\n",
    "\n",
    "    def u(self, c):\n",
    "        return c**self.alpha\n",
    "\n",
    "    def populate_R(self):\n",
    "\n",
    "        for s in range(self.n):\n",
    "            for a in range(self.m):\n",
    "                self.R[s, a] = self.u(s - a) if a <= s else -np.inf\n",
    "\n",
    "    def populate_Q(self):\n",
    "\n",
    "        for a in range(self.m):\n",
    "            self.Q[:, a, a:(a + self.B + 1)] = 1.0 / (self.B + 1)\n",
    "\n",
    "# Exact LP (GURPBI setting)\n",
    "def LP_approach(setting):\n",
    "    num_state = range(1, setting.num_states + 1)\n",
    "    num_action = range(1, setting.num_actions + 1)\n",
    "    LP = Model()\n",
    "    \n",
    "    # solve\n",
    "    x = {}\n",
    "    for i in num_state:\n",
    "        x[i ] = LP.addVar(vtype = \"C\", name = \"x(%s)\" %(i))\n",
    "    LP.update()\n",
    "    \n",
    "    for i in num_state:\n",
    "        for j in num_action:\n",
    "            LP.addConstr((x[i] - setting.beta * quicksum(setting.Q[i-1, j-1, k-1] * x[k] for k in num_state)) >= setting.R[i-1,j-1])\n",
    "            \n",
    "    LP.setObjective(quicksum(x[i] for i in num_state))\n",
    "    LP.params.OutputFlag = 0\n",
    "    LP.optimize()\n",
    "    \n",
    "    # v\n",
    "    v = np.empty(setting.num_states)\n",
    "    count = 0\n",
    "    for value in LP.getVars():\n",
    "        v[count] = value.X\n",
    "        count += 1\n",
    "        \n",
    "    # sigma\n",
    "    sigma = setting.compute_greedy(v)\n",
    "    \n",
    "    #result\n",
    "    res = qe.markov.ddp.DPSolveResult(v=v, sigma = sigma,\n",
    "                            mc=setting.controlled_mc(sigma),\n",
    "                            method = 'Exact Linear Programming (GUROBI setting)',\n",
    "                            )\n",
    "    \n",
    "    \n",
    "    return res\n",
    "\n",
    "    \n",
    "# linear programming time\n",
    "def elapse_LP(setting):\n",
    "    \n",
    "    # setting\n",
    "    start1 = time.time()\n",
    "    num_state = range(1, setting.num_states + 1)\n",
    "    num_action = range(1, setting.num_actions + 1)\n",
    "    LP = Model()\n",
    "    \n",
    "    x = {}\n",
    "    for i in num_state:\n",
    "        x[i ] = LP.addVar(vtype = \"C\", name = \"x(%s)\" %(i))\n",
    "    LP.update()\n",
    "    \n",
    "    for i in num_state:\n",
    "        for j in num_action:\n",
    "            LP.addConstr((x[i] - setting.beta * quicksum(setting.Q[i-1, j-1, k-1] * x[k] for k in num_state)) >= setting.R[i-1,j-1])\n",
    "            \n",
    "    LP.setObjective(quicksum(x[i] for i in num_state))\n",
    "    LP.params.OutputFlag = 0\n",
    "    print \"setting time\", time.time() - start1\n",
    "    \n",
    "    # optimization time\n",
    "    start2 = time.time()\n",
    "    LP.optimize()\n",
    "    print \"optimization time\", time.time() - start2\n",
    "    \n",
    "    # preparing for answer_ value function\n",
    "    start3 = time.time()\n",
    "    v = np.empty(setting.num_states)\n",
    "    count = 0\n",
    "    for value in LP.getVars():\n",
    "        v[count] = value.X\n",
    "        count += 1\n",
    "    print \"preparing for answer_ value function\", time.time() - start3\n",
    "        \n",
    "    # preparing for answer_ policy function\n",
    "    start4 = time.time()\n",
    "    sigma = setting.compute_greedy(v)\n",
    "    print \"preparing for answer_ policy function\", time.time() - start4\n",
    "    \n",
    "    #result\n",
    "    res = qe.markov.ddp.DPSolveResult(v=v, sigma = sigma,\n",
    "                            mc=setting.controlled_mc(sigma),\n",
    "                            method = 'Exact Linear Programming (GUROBI setting)',\n",
    "                            )\n",
    "\n",
    "# value function time\n",
    "def elapse_value(setting):\n",
    "    start = time.time()\n",
    "    setting.solve(method='value_iteration')\n",
    "    elapsed_time = time.time() - start\n",
    "    return elapsed_time\n",
    "\n",
    "# plot time until num_state = n\n",
    "def plot_graph(n, shock, discount):\n",
    "    a = [None] * n\n",
    "    b = [None] * n\n",
    "    for i in range(n):\n",
    "        h = SimpleOG(M = i, B = shock, beta = discount)\n",
    "        d = qe.markov.DiscreteDP(h.R, h.Q, h.beta)\n",
    "        a[i] = elapse_LP(d)\n",
    "        b[i] = elapse_value(d)\n",
    "\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    \n",
    "# time changing beta LP\n",
    "def plot_beta_LP(shock, state, betas):\n",
    "    n = [None] * len(betas)\n",
    "    for i, value in enumerate(betas):\n",
    "        model = SimpleOG(B = shock, M = state, beta = value)\n",
    "        ddp = qe.markov.DiscreteDP(model.R, model.Q, model.beta)\n",
    "        n[i] = elapse_LP(ddp)\n",
    "    plt.plot(n)\n",
    "    \n",
    "#time changing beta value function\n",
    "def plot_beta_value(shock, state, betas):\n",
    "    n = [None] * len(betas)\n",
    "    for i, value in enumerate(betas):\n",
    "        model = SimpleOG(B = shock, M = state, beta = value)\n",
    "        ddp = qe.markov.DiscreteDP(model.R, model.Q, model.beta)\n",
    "        n[i] = elapse_value(ddp)\n",
    "    plt.plot(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting time 7.82762122154\n",
      "optimization time 0.031359910965\n",
      "preparing for answer_ value function 0.000164985656738\n",
      "preparing for answer_ policy function 0.000349998474121\n"
     ]
    }
   ],
   "source": [
    "# ということでsettingの問題です。\n",
    "model = SimpleOG(B = 10, M = 50, beta = 0.99)\n",
    "ddp = qe.markov.DiscreteDP(model.R, model.Q, model.beta)\n",
    "elapse_LP(ddp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# より詳しく時間を分けます\n",
    "def elapse_LP_div(setting):\n",
    "    \n",
    "    # setting_model\n",
    "    start1 = time.time()\n",
    "    num_state = range(1, setting.num_states + 1)\n",
    "    num_action = range(1, setting.num_actions + 1)\n",
    "    LP = Model()\n",
    "    print \"setting_model\", time.time() - start1\n",
    "    \n",
    "    # setting_vars\n",
    "    start5 = time.time()\n",
    "    x = {}\n",
    "    for i in num_state:\n",
    "        x[i ] = LP.addVar(vtype = \"C\", name = \"x(%s)\" %(i))\n",
    "    LP.update()\n",
    "    print \"setting_vars\", time.time() - start5\n",
    "    \n",
    "    # setting const\n",
    "    start6 = time.time()\n",
    "    for i in num_state:\n",
    "        for j in num_action:\n",
    "            LP.addConstr((x[i] - setting.beta * quicksum(setting.Q[i-1, j-1, k-1] * x[k] for k in num_state)) >= setting.R[i-1,j-1])\n",
    "    print \"setting const\", time.time() - start6\n",
    "    \n",
    "    \n",
    "    # setting obj\n",
    "    start7 = time.time()\n",
    "    LP.setObjective(quicksum(x[i] for i in num_state))\n",
    "    LP.params.OutputFlag = 0\n",
    "    print \"setting obj\", time.time() - start7\n",
    "    \n",
    "    \n",
    "    # optimization time\n",
    "    start2 = time.time()\n",
    "    LP.optimize()\n",
    "    print \"optimization time\", time.time() - start2\n",
    "    \n",
    "    # preparing for answer_ value function\n",
    "    start3 = time.time()\n",
    "    v = np.empty(setting.num_states)\n",
    "    count = 0\n",
    "    for value in LP.getVars():\n",
    "        v[count] = value.X\n",
    "        count += 1\n",
    "    print \"preparing for answer_ value function\", time.time() - start3\n",
    "        \n",
    "    # preparing for answer_ policy function\n",
    "    start4 = time.time()\n",
    "    sigma = setting.compute_greedy(v)\n",
    "    print \"preparing for answer_ policy function\", time.time() - start4\n",
    "    \n",
    "    #result\n",
    "    res = qe.markov.ddp.DPSolveResult(v=v, sigma = sigma,\n",
    "                            mc=setting.controlled_mc(sigma),\n",
    "                            method = 'Exact Linear Programming (GUROBI setting)',\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting_model 8.79764556885e-05\n",
      "setting_vars 0.000349044799805\n",
      "setting const 8.07694411278\n",
      "setting obj 0.000422954559326\n",
      "optimization time 0.0308258533478\n",
      "preparing for answer_ value function 0.000264883041382\n",
      "preparing for answer_ policy function 0.000499963760376\n"
     ]
    }
   ],
   "source": [
    "# 一回目\n",
    "elapse_LP_div(ddp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting_model 0.00010085105896\n",
      "setting_vars 0.000320911407471\n",
      "setting const 7.98858618736\n",
      "setting obj 0.000387907028198\n",
      "optimization time 0.0295398235321\n",
      "preparing for answer_ value function 0.000235080718994\n",
      "preparing for answer_ policy function 0.000458955764771\n"
     ]
    }
   ],
   "source": [
    "# 二回目\n",
    "elapse_LP_div(ddp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ということでやはり制約条件の追加が遅いです。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
